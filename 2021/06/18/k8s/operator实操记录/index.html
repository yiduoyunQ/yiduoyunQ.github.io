<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="yiduoyunQ">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>yiduoyunQ</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">yiduoyunQ</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">yiduoyunQ</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    <meta name="referrer" content="no-referrer" />




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title"></h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-06-18
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    3.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="operator实操记录"><a href="#operator实操记录" class="headerlink" title="operator实操记录"></a>operator实操记录</h1><h2 id="0-部署环境"><a href="#0-部署环境" class="headerlink" title="0. 部署环境"></a>0. 部署环境</h2><ul>
<li>8C16G VM * 3</li>
<li>docker version: 20.10.6</li>
<li>kubeadm version: v1.21.1</li>
</ul>
<table>
<thead>
<tr>
<th>role</th>
<th>ip</th>
</tr>
</thead>
<tbody><tr>
<td>worker</td>
<td>172.16.6.186</td>
</tr>
<tr>
<td>worker</td>
<td>172.16.6.187</td>
</tr>
<tr>
<td>master</td>
<td>172.16.6.188</td>
</tr>
</tbody></table>
<h3 id="firewall"><a href="#firewall" class="headerlink" title="firewall"></a>firewall</h3><p>systemctl status firewalld<br>systemctl stop firewalld</p>
<h3 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h3><p>iptables -P FORWARD ACCEPT<br>iptables-save</p>
<h3 id="disable-SELinux"><a href="#disable-SELinux" class="headerlink" title="disable SELinux"></a>disable SELinux</h3><p>setenforce 0<br>/etc/selinux/config disable SELinux</p>
<h3 id="disable-swap"><a href="#disable-swap" class="headerlink" title="disable swap"></a>disable swap</h3><p>sync; swapoff -a; sync;<br>/etc/fstab 注释 swap</p>
<h3 id="sysctl"><a href="#sysctl" class="headerlink" title="sysctl"></a>sysctl</h3><p>modprobe br_netfilter</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.core.somaxconn = 32768
vm.swappiness = 0
net.ipv4.tcp_syncookies = 0
net.ipv4.ip_forward = 1
fs.file-max = 1000000
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 1024
net.ipv4.conf.all.rp_filter = 1
net.ipv4.neigh.default.gc_thresh1 = 80000
net.ipv4.neigh.default.gc_thresh2 = 90000
net.ipv4.neigh.default.gc_thresh3 = 100000
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>sysctl –system</p>
<h3 id="irqbalance"><a href="#irqbalance" class="headerlink" title="irqbalance"></a>irqbalance</h3><p>systemctl enable irqbalance<br>systemctl start irqbalance<br>systemctl status irqbalance</p>
<h3 id="CPUFreq-performance"><a href="#CPUFreq-performance" class="headerlink" title="CPUFreq performance"></a>CPUFreq performance</h3><p>虚拟机可跳过此步<br>cpupower frequency-info<br>cpupower frequency-set –governor performance</p>
<h3 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h3><p>ulimit -a<br>按需调整 /etc/security/limits.conf</p>
<h3 id="aliyun-repo"><a href="#aliyun-repo" class="headerlink" title="aliyun repo"></a>aliyun repo</h3><p>[root@centos76_vm ~]# yum-config-manager –add-repo <a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo">https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a><br>[root@centos76_vm ~]# cat /etc/yum.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/">https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</a><br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg">https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</a> <a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg">https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</a></p>
<h2 id="1-install-docker"><a href="#1-install-docker" class="headerlink" title="1. install docker"></a>1. install docker</h2><p>yum install -y yum-utils<br>yum install -y docker-ce</p>
<p>修改 cgroup driver 为 systemd</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "data-root": "/home/docker"
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>systemctl enable docker</p>
<blockquote>
<p>若docker service已启动，可通过 systemctl status docker | grep -i docker.service 确认 service 文件路径</p>
</blockquote>
<p>修改 /usr/lib/systemd/system/docker.service<br>LimitNOFILE=1048576</p>
<p>systemctl start/restart docker 生效配置</p>
<h2 id="2-install-kubeadm"><a href="#2-install-kubeadm" class="headerlink" title="2. install kubeadm"></a>2. install kubeadm</h2><p>yum install -y kubeadm<br>systemctl enable kubelet<br>systemctl start kubelet</p>
<p>安装后可用以下命令<br>kubeadm  kubectl  kubelet</p>
<h2 id="3-kubeadm-init-join"><a href="#3-kubeadm-init-join" class="headerlink" title="3. kubeadm init/join"></a>3. kubeadm init/join</h2><p>可手动指定国内仓库镜像， 通过 config pull image 到本地<br>kubeadm config images pull –image-repository=k8s.gcr.io/</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# docker images | grep k8s.gcr.io
k8s.gcr.io/kube-apiserver                                         v1.21.1    771ffcf9ca63   11 days ago    126MB
k8s.gcr.io/kube-controller-manager                                v1.21.1    e16544fd47b0   11 days ago    120MB
k8s.gcr.io/kube-proxy                                             v1.21.1    4359e752b596   11 days ago    131MB
k8s.gcr.io/kube-scheduler                                         v1.21.1    a4183b88f6e6   11 days ago    50.6MB
k8s.gcr.io/pause                                                  3.4.1      0f8457a4c2ec   4 months ago   683kB
k8s.gcr.io/coredns/coredns                                        v1.8.0     296a6d5035e2   7 months ago   42.5MB
k8s.gcr.io/etcd                                                   3.4.13-0   0369cf4303ff   8 months ago   253MB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="3-1-master"><a href="#3-1-master" class="headerlink" title="3.1 master"></a>3.1 master</h3><p><strong>–pod-network-cidr</strong> 根据下面安装 cni flannel deploy yaml 中 network 调整， 默认值为 <a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml#L128">10.244.0.0/16</a><br>若有多 Kubernetes 集群部署 tidb-cluster 需求，手动设定 dns 使用参数 –service-dns-domain</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init --node-name='172.16.6.188' --pod-network-cidr=10.244.0.0/16
...
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.6.188:6443 --token sd2lhv.7hbkznxww3eknymv \
	--discovery-token-ca-cert-hash sha256:86554a381b1d023d6812002060c6e70e00cf19c8a7a7e5e2a37394630cb874a2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>根据提示， 将 <code>export KUBECONFIG=/etc/kubernetes/admin.conf</code> 添加至 profile<br>worker node 需要手动拷贝 master node 上该文件 &amp; export</p>
<p>允许 master 节点调度 Pod</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl taint node 172.16.6.188 node-role.kubernetes.io/master:NoSchedule-
node/172.16.6.188 untainted<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="3-2-worker"><a href="#3-2-worker" class="headerlink" title="3.2 worker"></a>3.2 worker</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm join 172.16.6.188:6443 --token sd2lhv.7hbkznxww3eknymv \
	--discovery-token-ca-cert-hash sha256:86554a381b1d023d6812002060c6e70e00cf19c8a7a7e5e2a37394630cb874a2 \
    --node-name='172.16.6.187'

[root@centos76_vm ~]# kubectl get no -A
NAME           STATUS      ROLES                  AGE     VERSION
172.16.6.186   NotReady    &lt;none&gt;                 3m12s   v1.21.1
172.16.6.187   NotReady    &lt;none&gt;                 20h     v1.21.1
172.16.6.188   NotReady    control-plane,master   21h     v1.21.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>安装 CNI 插件 flannel</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created

[root@centos76_vm ~]# kubectl get po -A | grep flannel
kube-system   kube-flannel-ds-k6n6t                  1/1     Running   0          2d20h
kube-system   kube-flannel-ds-n26qj                  1/1     Running   0          3d17h
kube-system   kube-flannel-ds-zvv5f                  1/1     Running   0          3d16h<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-部署-operator"><a href="#4-部署-operator" class="headerlink" title="4. 部署 operator"></a>4. <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/deploy-tidb-operator">部署 operator</a></h2><h2 id="5-部署-tidb-cluster1"><a href="#5-部署-tidb-cluster1" class="headerlink" title="5. 部署 tidb-cluster1"></a>5. <a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml">部署 tidb-cluster1</a></h2><p>image 可以换成 uhub.service.ucloud.cn/pingcap/{pd/tikv/tidb}<br><strong>创建 pv 挂载点需在所有 node 上执行，否则 node 上将没有 pv 生成，无法满足 tikv 调度</strong></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 虚拟机的/home 在 /dev/sda2
# 使用 --bind 创建挂载点用于 pv disk
DISK_UUID=$(blkid -s UUID -o value /dev/sda2)
for i in $(seq 1 3); do
  mkdir -p /home/mnt/${DISK_UUID}/vol${i}
  mkdir -p /home/mnt/disks/${DISK_UUID}_vol${i}
  mount --bind /home/mnt/${DISK_UUID}/vol${i} /home/mnt/disks/${DISK_UUID}_vol${i}
done

# 使用operator provisioner，修改路径为 /home/mnt/disks
[root@centos76_vm ~]# wget https://raw.githubusercontent.com/pingcap/tidb-operator/v1.1.12/manifests/local-dind/local-volume-provisioner.yaml
[root@centos76_vm ~]# kubectl apply -f local-volume-provisioner.yaml

# 设为 default sc
[root@centos76_vm ~]# kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

[root@centos76_vm ~]# kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
local-pv-1799bdea   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-18a6c0a5   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-2b734959   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-3126cf18   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-a7b93e40   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-ae49967b   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-b6680abf   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-c139b536   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-ef0862f3   446Gi      RWO            Delete           Available           local-storage            9s

[root@centos76_vm ~]# kubectl create ns tidb-cluster1
[root@centos76_vm ~]# kubectl -n tidb-cluster1 apply -f tidb-cluster-1.yaml
[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-c9d6c97f4-pxtq2   1/1     Running   0          58s   10.244.2.126   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                        1/1     Running   0          58s   10.244.1.67    172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                      1/1     Running   0          17s   10.244.2.127   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                      1/1     Running   0          17s   10.244.2.128   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                      1/1     Running   0          17s   10.244.2.129   172.16.6.186   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="6-调度-tikv-pod"><a href="#6-调度-tikv-pod" class="headerlink" title="6. 调度 tikv pod"></a>6. 调度 tikv pod</h2><blockquote>
<p>官网上 operator v1.1.12 版本没有 topologySpreadConstraints 功能， 需使用最新版本 v1.2.0-rc.1</p>
</blockquote>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/v1.2.0-rc.1/manifests/crd.yaml
[root@centos76_vm ~]# mkdir -p ${HOME}/tidb-operator &amp;&amp; \
helm inspect values pingcap/tidb-operator --version=v1.2.0-rc.1 &gt; ${HOME}/tidb-operator/values-tidb-operator.yaml
[root@centos76_vm ~]# helm install tidb-operator pingcap/tidb-operator --namespace tidb-admin1 --version v1.2.0-rc.1 -f ${HOME}/tidb-operator/values-tidb-operator.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="6-1-通过-tidb-scheduler-调度"><a href="#6-1-通过-tidb-scheduler-调度" class="headerlink" title="6.1 通过 tidb-scheduler 调度"></a>6.1 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/tidb-scheduler#tidb-%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6%E9%9C%80%E6%B1%82">通过 tidb-scheduler 调度</a></h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# cat tidb-cluster-1.yaml | grep schedulerName
  schedulerName: "tidb-scheduler"
[root@centos76_vm ~]# cat tidb-cluster-1.yaml | grep annotations -A 1
  annotations:
    pingcap.com/ha-topology-key: kubernetes.io/hostname
[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-5df6575-kmjzc   1/1     Running   0          7m      10.244.2.13   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                      1/1     Running   0          7m      10.244.2.14   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tidb-0                    2/2     Running   0          6m9s    10.244.1.15   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                    1/1     Running   0          6m48s   10.244.1.14   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                    1/1     Running   0          6m48s   10.244.2.15   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                    1/1     Running   0          6m48s   10.244.0.4    172.16.6.188   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="6-2-通过-topologySpreadConstraints-调度"><a href="#6-2-通过-topologySpreadConstraints-调度" class="headerlink" title="6.2 通过 topologySpreadConstraints 调度"></a>6.2 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/configure-a-tidb-cluster#%E9%80%9A%E8%BF%87-topologyspreadconstraints-%E5%AE%9E%E7%8E%B0-pod-%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83">通过 topologySpreadConstraints 调度</a></h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# cat tidb-cluster-1.yaml  | grep topologySpreadConstraints -A 2
    topologySpreadConstraints:
    - topologyKey: kubernetes.io/hostname
    - topologyKey: zone
[root@centos76_vm ~]# kubectl patch no/172.16.6.186 -p '{"metadata":{"labels":{"zone":"z1"}}}'
[root@centos76_vm ~]# kubectl patch no/172.16.6.187 -p '{"metadata":{"labels":{"zone":"z2"}}}'
[root@centos76_vm ~]# kubectl patch no/172.16.6.188 -p '{"metadata":{"labels":{"zone":"z3"}}}'

[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-7cd58ddbc-nd7h2   1/1     Running   0          46s   10.244.2.33   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                        1/1     Running   0          45s   10.244.2.34   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                      1/1     Running   0          18s   10.244.1.33   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                      1/1     Running   0          18s   10.244.2.35   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                      1/1     Running   0          18s   10.244.0.10   172.16.6.188   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="6-3-通过-affinity-调度"><a href="#6-3-通过-affinity-调度" class="headerlink" title="6.3 通过 affinity 调度"></a>6.3 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/configure-a-tidb-cluster#%E9%80%9A%E8%BF%87-affinity-%E8%B0%83%E5%BA%A6%E5%AE%9E%E4%BE%8B">通过 affinity 调度</a></h3><h2 id="7-heterogeneous-在线迁移步骤"><a href="#7-heterogeneous-在线迁移步骤" class="headerlink" title="7. heterogeneous 在线迁移步骤"></a>7. heterogeneous 在线迁移步骤</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 重建 cluster1
sh /root/recreate.sh

# tidb node port
kubectl get svc -A | grep cluster1-tidb

# sysbench prepare &amp; analyze table

# 创建 monitor (在 new-cluster1 apply 后 scheduler 才会创建 monitor pod)
kubectl -n tidb-cluster1 apply -f /root/tidb-monitor.yaml

# sysbench run on cluster1.tidb &amp; 观察 sysbench QPS

# 创建 new-cluster1
kubectl -n tidb-cluster1 apply -f /root/new-tidb-cluster-1.yaml

# grafana node port &amp; 观察 grafana 曲线
kubectl get svc -A | grep grafana

# 扩容 new-cluster1 tc.spec.tikv.replicas 到 3
kubectl -n tidb-cluster1 edit tc/new-cluster1

# 观察 grafana 曲线 &amp; 等待 region 均衡

# 调整 sysbench run on new-cluster1.tidb (应用指向新 tidb)

# 缩容 cluster1 的 tidb 和 tikv &amp; 观察 grafana
kubectl -n tidb-cluster1 edit tc/cluster1

# transfer leader
kubectl -n tidb-cluster1 exec -it cluster1-pd-0 -- ./pd-ctl member leader transfer new-cluster1-pd-0

# 缩容 cluster1 的 pd
kubectl -n tidb-cluster1 edit tc/cluster1

# 删除 cluster1 集群 (discovery pod)
kubectl -n tidb-cluster1 delete tc/cluster1

# 调整 monitor spec.clusters.name 中去除 cluster1 (monitor pod 会重建)
kubectl -n tidb-cluster1 edit tm/basic

# 迁移完成<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="9-问题记录"><a href="#9-问题记录" class="headerlink" title="9. 问题记录"></a>9. 问题记录</h2><h3 id="docker-change-cgroup-driver"><a href="#docker-change-cgroup-driver" class="headerlink" title="docker change cgroup driver"></a>docker change cgroup driver</h3><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker">change runtime cgroup driver 为 systemd</a></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# docker info |grep cgroup
 Cgroup Driver: cgroupfs

[root@centos76_vm ~]# cat /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "data-root": "/home/docker"
}

[root@centos76_vm ~]# docker info |grep systemd
 Cgroup Driver: systemd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="kubeadm-init-报-nodeRegistration-name-Invalid-value"><a href="#kubeadm-init-报-nodeRegistration-name-Invalid-value" class="headerlink" title="kubeadm init 报 nodeRegistration.name: Invalid value"></a>kubeadm init 报 nodeRegistration.name: Invalid value</h3><p>VM 的 hostname 统一为 centos76_vm， 不符合 validation<br>临时 workaround 增加 –node-name=’172.16.6.188’<br><strong>实际环境应不会有该问题， kubeadm config images pull 暂时未找到 workaround</strong></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init
nodeRegistration.name: Invalid value: "centos76_vm": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
To see the stack trace of this error execute with --v=5 or higher

kubeadm init --node-name='172.16.6.188' --v 8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="已有-kubeadm-残留信息"><a href="#已有-kubeadm-残留信息" class="headerlink" title="已有 kubeadm 残留信息"></a>已有 kubeadm 残留信息</h3><p>使用 kubeadm reset 清理</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init --node-name='172.16.6.188'
[init] Using Kubernetes version: v1.21.1
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty


[root@centos76_vm ~]# kubeadm reset
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0524 16:39:28.363100    4665 reset.go:99] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get "https://172.16.6.188:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 172.16.6.188:6443: connect: connection refused
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0524 16:39:51.168162    4665 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>kubeadm reset 不会清理 iptables 和 cni 等信息， 手动清理</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">iptables -F

rm -rf /var/lib/cni/
rm -rf /etc/cni
ifconfig cni0 down
ifconfig flannel.1 down
ip link delete cni0
ip link delete flannel.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="k8s-coredns-pod-pending"><a href="#k8s-coredns-pod-pending" class="headerlink" title="k8s coredns pod pending"></a>k8s coredns pod pending</h3><p>若没安装 cni plugin 或 不指定/指定错误 <strong>–pod-network-cidr</strong>, k8s 默认 coredns 处于 Pending 状态</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A | grep Pending
kube-system   coredns-558bd4d5db-724kx               0/1     Pending   0          14m
kube-system   coredns-558bd4d5db-kkkmm               0/1     Pending   0          14m

[root@centos76_vm ~]# systemctl status kubelet.service  -l | tail -n 2
5月 24 18:13:58 centos76_vm kubelet[25338]: I0524 18:13:58.262707   25338 cni.go:239] "Unable to update cni config" err="no networks found in /etc/cni/net.d"
5月 24 18:13:59 centos76_vm kubelet[25338]: E0524 18:13:59.418055   25338 kubelet.go:2211] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"

[root@centos76_vm ~]# systemctl status kubelet.service  -l | tail -n 2
5月 24 18:37:41 centos76_vm kubelet[19932]: I0524 18:37:41.912576   19932 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="47aeebf532ce8cf03f44ef44ffa5e07cf627a896448de49a4d018c0007e04e56"
5月 24 18:37:41 centos76_vm kubelet[19932]: I0524 18:37:41.914559   19932 cni.go:333] "CNI failed to retrieve network namespace path" err="cannot find network namespace for the terminated container \"47aeebf532ce8cf03f44ef44ffa5e07cf627a896448de49a4d018c0007e04e56\""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="kubectl-连不上-master-api-server"><a href="#kubectl-连不上-master-api-server" class="headerlink" title="kubectl 连不上 {master} api server"></a>kubectl 连不上 {master} api server</h3><p>copy master /etc/kubernetes/admin.conf to worker node</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A
The connection to the server localhost:8080 was refused - did you specify the right host or port?

[root@centos76_vm ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; ~/.bash_profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="tidb-pd-pod-pending"><a href="#tidb-pd-pod-pending" class="headerlink" title="tidb pd pod pending"></a>tidb pd pod pending</h3><p>查看 log 为 pv FailedBinding， 参考使用 operator provisioner</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl -n tidb-cluster1 describe pvc/pd-cluster1-pd-0 | tail -n 3
  Type    Reason         Age                   From                         Message
  ----    ------         ----                  ----                         -------
  Normal  FailedBinding  11s (x23 over 5m27s)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="tikv-pod-反复-CrashLoopBackOff-Error"><a href="#tikv-pod-反复-CrashLoopBackOff-Error" class="headerlink" title="tikv pod 反复 CrashLoopBackOff/Error"></a>tikv pod 反复 CrashLoopBackOff/Error</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A |grep cluster1-tikv-3
tidb-cluster1   cluster1-tikv-3                            0/1     CrashLoopBackOff   6          8m40s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/exceptions#tikv-store-%E5%BC%82%E5%B8%B8%E8%BF%9B%E5%85%A5-tombstone-%E7%8A%B6%E6%80%81">并发进行 tikv 扩缩容可能导致，参考解决</a></p>
<h3 id="tikv-pod-logs-报错-store-is-tombstone"><a href="#tikv-pod-logs-报错-store-is-tombstone" class="headerlink" title="tikv pod logs 报错 store is tombstone"></a>tikv pod logs 报错 store is tombstone</h3><p>现象：查看报错为重复使用 pv 的 store id 已处于 tombstone 状态<br>过程：反复对 tikv 进行扩容和缩容，过程中手动 delete pv。 通过 kubectl -n tidb-cluster1 describe tc cluster1， 查看当前存在 tombstone 状态的 store<br>原因：正常 pv 不能手动 delete，需要配合 sc 由 pvc 进行删除。同时 pv 对应的本地盘数据需手动清理。 由于未清理本地磁盘的数据，重建tikv pod时使用了原有数据，导致 store id 重复<br>解决：手动清理磁盘数据； 也可通过 pd-ctl store remove-tombstone 手动清理 tombstone 状态的 store</p>
<h3 id="operator-v1-1-12-配置-topologySpreadConstraints-无效果"><a href="#operator-v1-1-12-配置-topologySpreadConstraints-无效果" class="headerlink" title="operator v1.1.12 配置 topologySpreadConstraints 无效果"></a>operator v1.1.12 配置 topologySpreadConstraints 无效果</h3><p>确认 v1.1.12 tc.spec 代码中无该变量，未实现该功能</p>
<h2 id="99-tidb-集群创建简化命令"><a href="#99-tidb-集群创建简化命令" class="headerlink" title="99. tidb 集群创建简化命令"></a>99. tidb 集群创建简化命令</h2><ul>
<li>完成 k8s node cluster 创建</li>
<li>完成 helm3 安装</li>
</ul>
<h3 id="pv-disk"><a href="#pv-disk" class="headerlink" title="pv disk"></a>pv disk</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">DISK_UUID=$(blkid -s UUID -o value /dev/sda2)
for i in $(seq 1 3); do
  mkdir -p /home/mnt/${DISK_UUID}/vol${i}
  mkdir -p /home/mnt/disks/${DISK_UUID}_vol${i}
  mount --bind /home/mnt/${DISK_UUID}/vol${i} /home/mnt/disks/${DISK_UUID}_vol${i}
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="provisioner"><a href="#provisioner" class="headerlink" title="provisioner"></a>provisioner</h3><blockquote>
<p>operator provisioner，修改路径为 /home/mnt/disks</p>
</blockquote>
<p>wget <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/local-dind/local-volume-provisioner.yaml">https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/local-dind/local-volume-provisioner.yaml</a></p>
<ul>
<li>tidb-cluster</li>
</ul>
<p>参考 github sample， 修改 namespace 等参数<br><a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml">https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml</a><br><a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/advanced/tidb-cluster.yaml">https://github.com/pingcap/tidb-operator/blob/master/examples/advanced/tidb-cluster.yaml</a></p>
<h3 id="重建集群脚本-root-recreate-sh"><a href="#重建集群脚本-root-recreate-sh" class="headerlink" title="重建集群脚本 /root/recreate.sh"></a>重建集群脚本 /root/recreate.sh</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#!/bin/bash

# 环境变量
tidb_cluster_ns=tidb-cluster1
operator_ns=tidb-admin1
operator_v=v1.2.0-rc.1

lvp_dir=/root/local-volume-provisioner.yaml
operator_dir=${HOME}/tidb-operator/values-tidb-operator.yaml
tidb_cluser_dir=/root/tidb-cluster-1.yaml

master=172.16.6.188
worker1=172.16.6.186
worker2=172.16.6.187

kubectl delete ns $tidb_cluster_ns
kubectl delete ns $operator_ns
kubectl delete -f $lvp_dir
for pv in `kubectl get pv | grep local-pv | awk '{print $1}'`
do
  kubectl delete pv/$pv
done

disk_clean () 
{
  ssh root@$1 "
  rm -f /tmp/disk_clean.sh
  echo '
  DISK_UUID=\$(blkid -s UUID -o value /dev/sda2)
  for i in \$(seq 1 3); do
    rm  -Rf /home/mnt/disks/\${DISK_UUID}_vol\${i}/*
  done
  ' &gt; /tmp/disk_clean.sh
  sh /tmp/disk_clean.sh
"
}

disk_clean $master
disk_clean $worker1
disk_clean $worker2

echo "*** finish clean ***"

kubectl create ns $tidb_cluster_ns
kubectl create ns $operator_ns

kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/crd.yaml
# 使用operator provisioner，修改路径为 /home/mnt/disks
rm -f /root/local-volume-provisioner.yaml
wget https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/local-dind/local-volume-provisioner.yaml -O /root/local-volume-provisioner.yaml
sed -i "s/\/mnt\/disks/\/home\/mnt\/disks/g" local-volume-provisioner.yaml
kubectl apply -f /root/local-volume-provisioner.yaml

# 按需修改 value
mkdir -p ${HOME}/tidb-operator &amp;&amp; \
helm inspect values pingcap/tidb-operator --version=$operator_v &gt; ${HOME}/tidb-operator/values-tidb-operator.yaml
helm install tidb-operator pingcap/tidb-operator --namespace $operator_ns --version $operator_v -f $operator_dir\
    --set operatorImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-operator:$operator_v \
    --set tidbBackupManagerImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-backup-manager:$operator_v \
    --set scheduler.kubeSchedulerImageName=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler

kubectl apply -f $lvp_dir
kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl -n $tidb_cluster_ns apply -f $tidb_cluser_dir

echo "*** finsh recreate ***"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">yiduoyunQ</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://yiduoyunQ.github.io/2021/06/18/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/">https://yiduoyunQ.github.io/2021/06/18/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">yiduoyunQ</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2021/06/18/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="">
                        
                        <span class="card-title"></span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-06-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            yiduoyunQ
                            
                        </span>
                    </div>
                </div>

                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/03/23/go/cond/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/13.jpg" class="responsive-img" alt="sync.cond">
                        
                        <span class="card-title">sync.cond</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-03-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/go/" class="post-category">
                                    go
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%BA%90%E7%A0%81/">
                        <span class="chip bg-color">源码</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2021</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">yiduoyunQ</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/yiduoyunQ" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:719547995@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=719547995" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 719547995" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
