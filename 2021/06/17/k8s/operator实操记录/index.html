<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yiduoyunq.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="0. 部署环境 8C16G VM * 3 docker version: 20.10.6 kubeadm version: v1.21.1     role ip    worker 172.16.6.186   worker 172.16.6.187   master 172.16.6.188   firewallcluster1-pd-0systemctl status firewalldsy">
<meta property="og:type" content="article">
<meta property="og:title" content="operator实操记录">
<meta property="og:url" content="https://yiduoyunq.github.io/2021/06/17/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="yiduoyunQ">
<meta property="og:description" content="0. 部署环境 8C16G VM * 3 docker version: 20.10.6 kubeadm version: v1.21.1     role ip    worker 172.16.6.186   worker 172.16.6.187   master 172.16.6.188   firewallcluster1-pd-0systemctl status firewalldsy">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-06-17T22:35:45.000Z">
<meta property="article:modified_time" content="2021-11-29T12:36:34.661Z">
<meta property="article:author" content="yiduoyunQ">
<meta property="article:tag" content="k8s">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yiduoyunq.github.io/2021/06/17/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>operator实操记录 | yiduoyunQ</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yiduoyunQ</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yiduoyunq.github.io/2021/06/17/k8s/operator%E5%AE%9E%E6%93%8D%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yiduoyunQ">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yiduoyunQ">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          operator实操记录
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-17 22:35:45" itemprop="dateCreated datePublished" datetime="2021-06-17T22:35:45+00:00">2021-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-29 12:36:34" itemprop="dateModified" datetime="2021-11-29T12:36:34+00:00">2021-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="0-部署环境"><a href="#0-部署环境" class="headerlink" title="0. 部署环境"></a>0. 部署环境</h2><ul>
<li>8C16G VM * 3</li>
<li>docker version: 20.10.6</li>
<li>kubeadm version: v1.21.1</li>
</ul>
<table>
<thead>
<tr>
<th>role</th>
<th>ip</th>
</tr>
</thead>
<tbody><tr>
<td>worker</td>
<td>172.16.6.186</td>
</tr>
<tr>
<td>worker</td>
<td>172.16.6.187</td>
</tr>
<tr>
<td>master</td>
<td>172.16.6.188</td>
</tr>
</tbody></table>
<h3 id="firewallcluster1-pd-0"><a href="#firewallcluster1-pd-0" class="headerlink" title="firewallcluster1-pd-0"></a>firewallcluster1-pd-0</h3><p>systemctl status firewalld<br>systemctl stop firewalld</p>
<h3 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h3><p>iptables -P FORWARD ACCEPT<br>iptables-save</p>
<h3 id="disable-SELinux"><a href="#disable-SELinux" class="headerlink" title="disable SELinux"></a>disable SELinux</h3><p>setenforce 0<br>/etc/selinux/config disable SELinux</p>
<h3 id="disable-swap"><a href="#disable-swap" class="headerlink" title="disable swap"></a>disable swap</h3><p>sync; swapoff -a; sync;<br>/etc/fstab 注释 swap</p>
<h3 id="sysctl"><a href="#sysctl" class="headerlink" title="sysctl"></a>sysctl</h3><p>modprobe br_netfilter</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.core.somaxconn = 32768
vm.swappiness = 0
net.ipv4.tcp_syncookies = 0
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
fs.file-max = 1000000
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 1024
net.ipv4.conf.all.rp_filter = 1
net.ipv4.neigh.default.gc_thresh1 = 80000
net.ipv4.neigh.default.gc_thresh2 = 90000
net.ipv4.neigh.default.gc_thresh3 = 100000
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>sysctl –system</p>
<h3 id="irqbalance"><a href="#irqbalance" class="headerlink" title="irqbalance"></a>irqbalance</h3><p>systemctl enable irqbalance<br>systemctl start irqbalance<br>systemctl status irqbalance</p>
<h3 id="CPUFreq-performance"><a href="#CPUFreq-performance" class="headerlink" title="CPUFreq performance"></a>CPUFreq performance</h3><p>虚拟机可跳过此步<br>cpupower frequency-info<br>cpupower frequency-set –governor performance</p>
<h3 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h3><p>ulimit -a<br>按需调整 /etc/security/limits.conf</p>
<h3 id="aliyun-repo"><a href="#aliyun-repo" class="headerlink" title="aliyun repo"></a>aliyun repo</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

cat &lt;&lt;EOF &gt;  /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="1-install-docker"><a href="#1-install-docker" class="headerlink" title="1. install docker"></a>1. install docker</h2><p>yum install -y yum-utils<br>yum install -y docker-ce</p>
<p>修改 cgroup driver 为 systemd</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cat &lt;&lt;EOF &gt;  /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "data-root": "/home/docker"
}
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>systemctl enable docker</p>
<blockquote>
<p>若docker service已启动，可通过 systemctl status docker | grep -i docker.service 确认 service 文件路径</p>
</blockquote>
<p>修改 /usr/lib/systemd/system/docker.service<br>LimitNOFILE=1048576</p>
<p>systemctl start/restart docker 生效配置</p>
<h2 id="2-install-kubeadm"><a href="#2-install-kubeadm" class="headerlink" title="2. install kubeadm"></a>2. install kubeadm</h2><p>yum install -y kubeadm<br>systemctl enable kubelet<br>systemctl start kubelet</p>
<p>安装后可用以下命令<br>kubeadm  kubectl  kubelet</p>
<h2 id="3-kubeadm-init-join"><a href="#3-kubeadm-init-join" class="headerlink" title="3. kubeadm init/join"></a>3. kubeadm init/join</h2><p>可手动指定国内仓库镜像， 通过 config pull image 到本地<br>kubeadm config images pull –image-repository=k8s.gcr.io/</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# docker images | grep k8s.gcr.io
k8s.gcr.io/kube-apiserver                                         v1.21.1    771ffcf9ca63   11 days ago    126MB
k8s.gcr.io/kube-controller-manager                                v1.21.1    e16544fd47b0   11 days ago    120MB
k8s.gcr.io/kube-proxy                                             v1.21.1    4359e752b596   11 days ago    131MB
k8s.gcr.io/kube-scheduler                                         v1.21.1    a4183b88f6e6   11 days ago    50.6MB
k8s.gcr.io/pause                                                  3.4.1      0f8457a4c2ec   4 months ago   683kB
k8s.gcr.io/coredns/coredns                                        v1.8.0     296a6d5035e2   7 months ago   42.5MB
k8s.gcr.io/etcd                                                   3.4.13-0   0369cf4303ff   8 months ago   253MB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="3-1-master"><a href="#3-1-master" class="headerlink" title="3.1 master"></a>3.1 master</h3><p><strong>–pod-network-cidr</strong> 根据下面安装 cni flannel deploy yaml 中 network 调整， 默认值为 <a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml#L128">10.244.0.0/16</a><br>若有多 Kubernetes 集群部署 tidb-cluster 需求，手动设定 dns 使用参数 –service-dns-domain</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init --node-name='172.16.6.188' --pod-network-cidr=10.244.0.0/16
...
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.6.188:6443 --token sd2lhv.7hbkznxww3eknymv \
	--discovery-token-ca-cert-hash sha256:86554a381b1d023d6812002060c6e70e00cf19c8a7a7e5e2a37394630cb874a2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>根据提示， 将 <code>export KUBECONFIG=/etc/kubernetes/admin.conf</code> 添加至 profile<br>worker node 需要手动拷贝 master node 上该文件 &amp; export</p>
<p>允许 master 节点调度 Pod</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl taint node 172.16.6.188 node-role.kubernetes.io/master:NoSchedule-
node/172.16.6.188 untainted<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="3-1-1-IPv6"><a href="#3-1-1-IPv6" class="headerlink" title="3.1.1 IPv6"></a>3.1.1 IPv6</h4><p>kubeadm init –apiserver-advertise-address=2600:1f13:c89:8900:150d:1ed:13c8:e90d</p>
<h3 id="3-2-worker"><a href="#3-2-worker" class="headerlink" title="3.2 worker"></a>3.2 worker</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm join 172.16.6.188:6443 --token sd2lhv.7hbkznxww3eknymv \
	--discovery-token-ca-cert-hash sha256:86554a381b1d023d6812002060c6e70e00cf19c8a7a7e5e2a37394630cb874a2 \
    --node-name='172.16.6.187'

[root@centos76_vm ~]# kubectl get no -A
NAME           STATUS      ROLES                  AGE     VERSION
172.16.6.186   NotReady    &lt;none&gt;                 3m12s   v1.21.1
172.16.6.187   NotReady    &lt;none&gt;                 20h     v1.21.1
172.16.6.188   NotReady    control-plane,master   21h     v1.21.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>安装 CNI 插件 flannel</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created

[root@centos76_vm ~]# kubectl get po -A | grep flannel
kube-system   kube-flannel-ds-k6n6t                  1/1     Running   0          2d20h
kube-system   kube-flannel-ds-n26qj                  1/1     Running   0          3d17h
kube-system   kube-flannel-ds-zvv5f                  1/1     Running   0          3d16h<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="4-部署-operator"><a href="#4-部署-operator" class="headerlink" title="4. 部署 operator"></a>4. <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/deploy-tidb-operator">部署 operator</a></h2><h2 id="5-部署-tidb-cluster1"><a href="#5-部署-tidb-cluster1" class="headerlink" title="5. 部署 tidb-cluster1"></a>5. <a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml">部署 tidb-cluster1</a></h2><p>image 可以换成 uhub.service.ucloud.cn/pingcap/{pd/tikv/tidb}<br><strong>创建 pv 挂载点需在所有 node 上执行，否则 node 上将没有 pv 生成，无法满足 tikv 调度</strong></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 虚拟机的/home 在 /dev/sda2
# 使用 --bind 创建挂载点用于 pv disk
DISK_UUID=$(blkid -s UUID -o value /dev/sda2)
for i in $(seq 1 3); do
  mkdir -p /home/mnt/${DISK_UUID}/vol${i}
  mkdir -p /home/mnt/disks/${DISK_UUID}_vol${i}
  mount --bind /home/mnt/${DISK_UUID}/vol${i} /home/mnt/disks/${DISK_UUID}_vol${i}
done

# 使用operator provisioner，修改路径为 /home/mnt/disks
[root@centos76_vm ~]# wget https://raw.githubusercontent.com/pingcap/tidb-operator/v1.1.12/manifests/local-dind/local-volume-provisioner.yaml
[root@centos76_vm ~]# kubectl apply -f local-volume-provisioner.yaml

# 设为 default sc
[root@centos76_vm ~]# kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

[root@centos76_vm ~]# kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
local-pv-1799bdea   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-18a6c0a5   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-2b734959   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-3126cf18   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-a7b93e40   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-ae49967b   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-b6680abf   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-c139b536   446Gi      RWO            Delete           Available           local-storage            9s
local-pv-ef0862f3   446Gi      RWO            Delete           Available           local-storage            9s

[root@centos76_vm ~]# kubectl create ns tidb-cluster1
[root@centos76_vm ~]# kubectl -n tidb-cluster1 apply -f tidb-cluster-1.yaml
[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-c9d6c97f4-pxtq2   1/1     Running   0          58s   10.244.2.126   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                        1/1     Running   0          58s   10.244.1.67    172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                      1/1     Running   0          17s   10.244.2.127   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                      1/1     Running   0          17s   10.244.2.128   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                      1/1     Running   0          17s   10.244.2.129   172.16.6.186   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="6-调度-tikv-pod"><a href="#6-调度-tikv-pod" class="headerlink" title="6. 调度 tikv pod"></a>6. 调度 tikv pod</h2><blockquote>
<p>官网上 operator v1.1.12 版本没有 topologySpreadConstraints 功能， 需使用最新版本 v1.2.0-rc.1</p>
</blockquote>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/v1.2.0-rc.1/manifests/crd.yaml
[root@centos76_vm ~]# mkdir -p ${HOME}/tidb-operator &amp;&amp; \
helm inspect values pingcap/tidb-operator --version=v1.2.0-rc.1 &gt; ${HOME}/tidb-operator/values-tidb-operator.yaml
[root@centos76_vm ~]# helm install tidb-operator pingcap/tidb-operator --namespace tidb-admin1 --version v1.2.0-rc.1 -f ${HOME}/tidb-operator/values-tidb-operator.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="6-1-通过-tidb-scheduler-调度"><a href="#6-1-通过-tidb-scheduler-调度" class="headerlink" title="6.1 通过 tidb-scheduler 调度"></a>6.1 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/tidb-scheduler#tidb-%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6%E9%9C%80%E6%B1%82">通过 tidb-scheduler 调度</a></h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# cat tidb-cluster-1.yaml | grep schedulerName
  schedulerName: "tidb-scheduler"
[root@centos76_vm ~]# cat tidb-cluster-1.yaml | grep annotations -A 1
  annotations:
    pingcap.com/ha-topology-key: kubernetes.io/hostname
[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-5df6575-kmjzc   1/1     Running   0          7m      10.244.2.13   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                      1/1     Running   0          7m      10.244.2.14   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tidb-0                    2/2     Running   0          6m9s    10.244.1.15   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                    1/1     Running   0          6m48s   10.244.1.14   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                    1/1     Running   0          6m48s   10.244.2.15   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                    1/1     Running   0          6m48s   10.244.0.4    172.16.6.188   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="6-2-通过-topologySpreadConstraints-调度"><a href="#6-2-通过-topologySpreadConstraints-调度" class="headerlink" title="6.2 通过 topologySpreadConstraints 调度"></a>6.2 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/configure-a-tidb-cluster#%E9%80%9A%E8%BF%87-topologyspreadconstraints-%E5%AE%9E%E7%8E%B0-pod-%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83">通过 topologySpreadConstraints 调度</a></h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# cat tidb-cluster-1.yaml  | grep topologySpreadConstraints -A 2
    topologySpreadConstraints:
    - topologyKey: kubernetes.io/hostname
    - topologyKey: zone
[root@centos76_vm ~]# kubectl patch no/172.16.6.186 -p '{"metadata":{"labels":{"zone":"z1"}}}'
[root@centos76_vm ~]# kubectl patch no/172.16.6.187 -p '{"metadata":{"labels":{"zone":"z2"}}}'
[root@centos76_vm ~]# kubectl patch no/172.16.6.188 -p '{"metadata":{"labels":{"zone":"z3"}}}'

[root@centos76_vm ~]# kubectl get po -n tidb-cluster1 -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
cluster1-discovery-7cd58ddbc-nd7h2   1/1     Running   0          46s   10.244.2.33   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-pd-0                        1/1     Running   0          45s   10.244.2.34   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-0                      1/1     Running   0          18s   10.244.1.33   172.16.6.186   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-1                      1/1     Running   0          18s   10.244.2.35   172.16.6.187   &lt;none&gt;           &lt;none&gt;
cluster1-tikv-2                      1/1     Running   0          18s   10.244.0.10   172.16.6.188   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="6-3-通过-affinity-调度"><a href="#6-3-通过-affinity-调度" class="headerlink" title="6.3 通过 affinity 调度"></a>6.3 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/configure-a-tidb-cluster#%E9%80%9A%E8%BF%87-affinity-%E8%B0%83%E5%BA%A6%E5%AE%9E%E4%BE%8B">通过 affinity 调度</a></h3><h2 id="7-heterogeneous-在线迁移步骤"><a href="#7-heterogeneous-在线迁移步骤" class="headerlink" title="7. heterogeneous 在线迁移步骤"></a>7. heterogeneous 在线迁移步骤</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 重建 cluster1
sh /root/recreate.sh

# tidb node port
kubectl get svc -A | grep cluster1-tidb

# sysbench prepare &amp; analyze table

# 创建 monitor (在 new-cluster1 apply 后 scheduler 才会创建 monitor pod)
kubectl -n tidb-cluster1 apply -f /root/tidb-monitor.yaml

# sysbench run on cluster1.tidb &amp; 观察 sysbench QPS

# 创建 new-cluster1
kubectl -n tidb-cluster1 apply -f /root/new-tidb-cluster-1.yaml

# grafana node port &amp; 观察 grafana 曲线
kubectl get svc -A | grep grafana

# 扩容 new-cluster1 tc.spec.tikv.replicas 到 3
kubectl -n tidb-cluster1 edit tc/new-cluster1

# 观察 grafana 曲线 &amp; 等待 region 均衡

# 调整 sysbench run on new-cluster1.tidb (应用指向新 tidb)

# 缩容 cluster1 的 tidb 和 tikv &amp; 观察 grafana
kubectl -n tidb-cluster1 edit tc/cluster1

# transfer leader
kubectl -n tidb-cluster1 exec -it cluster1-pd-0 -- ./pd-ctl member leader transfer new-cluster1-pd-0

# 缩容 cluster1 的 pd
kubectl -n tidb-cluster1 edit tc/cluster1

# 删除 cluster1 集群 (discovery pod)
kubectl -n tidb-cluster1 delete tc/cluster1

# 调整 monitor spec.clusters.name 中去除 cluster1 (monitor pod 会重建)
kubectl -n tidb-cluster1 edit tm/basic

# 迁移完成<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="7-1-grafana-多集群监控问题"><a href="#7-1-grafana-多集群监控问题" class="headerlink" title="7.1 grafana 多集群监控问题"></a>7.1 <a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/monitor-a-tidb-cluster#%E4%BD%BF%E7%94%A8-grafana-%E6%9F%A5%E7%9C%8B%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7">grafana 多集群监控问题</a></h3><p>也可以在 new-tc.metadata 增加:<br>labels:<br>  app.kubernetes.io/instance: {origin_cluster_name}</p>
<h2 id="9-问题记录"><a href="#9-问题记录" class="headerlink" title="9. 问题记录"></a>9. 问题记录</h2><h3 id="docker-change-cgroup-driver"><a href="#docker-change-cgroup-driver" class="headerlink" title="docker change cgroup driver"></a>docker change cgroup driver</h3><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker">change runtime cgroup driver 为 systemd</a></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# docker info |grep cgroup
 Cgroup Driver: cgroupfs

[root@centos76_vm ~]# cat /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "data-root": "/home/docker"
}

[root@centos76_vm ~]# docker info |grep systemd
 Cgroup Driver: systemd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="kubeadm-init-报-nodeRegistration-name-Invalid-value"><a href="#kubeadm-init-报-nodeRegistration-name-Invalid-value" class="headerlink" title="kubeadm init 报 nodeRegistration.name: Invalid value"></a>kubeadm init 报 nodeRegistration.name: Invalid value</h3><p>VM 的 hostname 统一为 centos76_vm， 不符合 validation<br>临时 workaround 增加 –node-name=’172.16.6.188’<br><strong>实际环境应不会有该问题， kubeadm config images pull 暂时未找到 workaround</strong></p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init
nodeRegistration.name: Invalid value: "centos76_vm": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
To see the stack trace of this error execute with --v=5 or higher

kubeadm init --node-name='172.16.6.188' --v 8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="已有-kubeadm-残留信息"><a href="#已有-kubeadm-残留信息" class="headerlink" title="已有 kubeadm 残留信息"></a>已有 kubeadm 残留信息</h3><p>使用 kubeadm reset 清理</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubeadm init --node-name='172.16.6.188'
[init] Using Kubernetes version: v1.21.1
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty

[root@centos76_vm ~]# kubeadm reset
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0524 16:39:28.363100    4665 reset.go:99] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get "https://172.16.6.188:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 172.16.6.188:6443: connect: connection refused
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0524 16:39:51.168162    4665 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>kubeadm reset 不会清理 iptables 和 cni 等信息， 手动清理</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">iptables -F

rm -rf /var/lib/cni/
rm -rf /etc/cni
ifconfig cni0 down
ifconfig flannel.1 down
ip link delete cni0
ip link delete flannel.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="k8s-coredns-pod-pending"><a href="#k8s-coredns-pod-pending" class="headerlink" title="k8s coredns pod pending"></a>k8s coredns pod pending</h3><p>若没安装 cni plugin 或 不指定/指定错误 <strong>–pod-network-cidr</strong>, k8s 默认 coredns 处于 Pending 状态</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A | grep Pending
kube-system   coredns-558bd4d5db-724kx               0/1     Pending   0          14m
kube-system   coredns-558bd4d5db-kkkmm               0/1     Pending   0          14m

[root@centos76_vm ~]# systemctl status kubelet.service  -l | tail -n 2
5月 24 18:13:58 centos76_vm kubelet[25338]: I0524 18:13:58.262707   25338 cni.go:239] "Unable to update cni config" err="no networks found in /etc/cni/net.d"
5月 24 18:13:59 centos76_vm kubelet[25338]: E0524 18:13:59.418055   25338 kubelet.go:2211] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"

[root@centos76_vm ~]# systemctl status kubelet.service  -l | tail -n 2
5月 24 18:37:41 centos76_vm kubelet[19932]: I0524 18:37:41.912576   19932 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="47aeebf532ce8cf03f44ef44ffa5e07cf627a896448de49a4d018c0007e04e56"
5月 24 18:37:41 centos76_vm kubelet[19932]: I0524 18:37:41.914559   19932 cni.go:333] "CNI failed to retrieve network namespace path" err="cannot find network namespace for the terminated container \"47aeebf532ce8cf03f44ef44ffa5e07cf627a896448de49a4d018c0007e04e56\""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="kubectl-连不上-master-api-server"><a href="#kubectl-连不上-master-api-server" class="headerlink" title="kubectl 连不上 {master} api server"></a>kubectl 连不上 {master} api server</h3><p>copy master /etc/kubernetes/admin.conf to worker node</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A
The connection to the server localhost:8080 was refused - did you specify the right host or port?

[root@centos76_vm ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; ~/.bash_profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="tidb-pd-pod-pending"><a href="#tidb-pd-pod-pending" class="headerlink" title="tidb pd pod pending"></a>tidb pd pod pending</h3><p>查看 log 为 pv FailedBinding， 参考使用 operator provisioner</p>
<pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl -n tidb-cluster1 describe pvc/pd-cluster1-pd-0 | tail -n 3
  Type    Reason         Age                   From                         Message
  ----    ------         ----                  ----                         -------
  Normal  FailedBinding  11s (x23 over 5m27s)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="tikv-pod-反复-CrashLoopBackOff-Error"><a href="#tikv-pod-反复-CrashLoopBackOff-Error" class="headerlink" title="tikv pod 反复 CrashLoopBackOff/Error"></a>tikv pod 反复 CrashLoopBackOff/Error</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">[root@centos76_vm ~]# kubectl get po -A |grep cluster1-tikv-3
tidb-cluster1   cluster1-tikv-3                            0/1     CrashLoopBackOff   6          8m40s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/exceptions#tikv-store-%E5%BC%82%E5%B8%B8%E8%BF%9B%E5%85%A5-tombstone-%E7%8A%B6%E6%80%81">并发进行 tikv 扩缩容可能导致，参考解决</a></p>
<h3 id="tikv-pod-logs-报错-store-is-tombstone"><a href="#tikv-pod-logs-报错-store-is-tombstone" class="headerlink" title="tikv pod logs 报错 store is tombstone"></a>tikv pod logs 报错 store is tombstone</h3><p>现象：查看报错为重复使用 pv 的 store id 已处于 tombstone 状态<br>过程：反复对 tikv 进行扩容和缩容，过程中手动 delete pv。 通过 kubectl -n tidb-cluster1 describe tc cluster1， 查看当前存在 tombstone 状态的 store<br>原因：正常 pv 不能手动 delete，需要配合 sc 由 pvc 进行删除。同时 pv 对应的本地盘数据需手动清理。 由于未清理本地磁盘的数据，重建tikv pod时使用了原有数据，导致 store id 重复<br>解决：手动清理磁盘数据； 也可通过 pd-ctl store remove-tombstone 手动清理 tombstone 状态的 store</p>
<h3 id="operator-v1-1-12-配置-topologySpreadConstraints-无效果"><a href="#operator-v1-1-12-配置-topologySpreadConstraints-无效果" class="headerlink" title="operator v1.1.12 配置 topologySpreadConstraints 无效果"></a>operator v1.1.12 配置 topologySpreadConstraints 无效果</h3><p>确认 v1.1.12 tc.spec 代码中无该变量，未实现该功能</p>
<h2 id="99-tidb-集群创建简化命令"><a href="#99-tidb-集群创建简化命令" class="headerlink" title="99. tidb 集群创建简化命令"></a>99. tidb 集群创建简化命令</h2><ul>
<li>完成 k8s node cluster 创建</li>
<li>完成 helm3 安装, helm repo add pingcap <a target="_blank" rel="noopener" href="https://charts.pingcap.org/">https://charts.pingcap.org/</a></li>
</ul>
<h3 id="mount-pv-disk"><a href="#mount-pv-disk" class="headerlink" title="mount pv disk"></a>mount pv disk</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">DISK_UUID=$(blkid -s UUID -o value /dev/sda2)
for i in $(seq 1 3); do
  mkdir -p /home/mnt/${DISK_UUID}/vol${i}
  mkdir -p /home/mnt/disks/${DISK_UUID}_vol${i}
  mount --bind /home/mnt/${DISK_UUID}/vol${i} /home/mnt/disks/${DISK_UUID}_vol${i}
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>tc yaml</li>
</ul>
<p>参考 github sample， 修改 namespace 等参数<br><a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml">https://github.com/pingcap/tidb-operator/blob/master/examples/multi-cluster/tidb-cluster-1.yaml</a><br><a target="_blank" rel="noopener" href="https://github.com/pingcap/tidb-operator/blob/master/examples/advanced/tidb-cluster.yaml">https://github.com/pingcap/tidb-operator/blob/master/examples/advanced/tidb-cluster.yaml</a></p>
<h3 id="重建集群脚本-root-recreate-sh"><a href="#重建集群脚本-root-recreate-sh" class="headerlink" title="重建集群脚本 /root/recreate.sh"></a>重建集群脚本 /root/recreate.sh</h3><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">#!/bin/bash

# 环境变量
tidb_cluster_ns=tidb-cluster1
operator_ns=tidb-admin1
operator_v=v1.2.0-rc.1

lvp_dir=/root/local-volume-provisioner.yaml
operator_dir=${HOME}/tidb-operator/values-tidb-operator.yaml
tidb_cluser_file=/root/tidb-cluster-1.yaml
tidb_monitor_file=/root/tidb-monitor.yaml

master=172.16.6.188
worker1=172.16.6.186
worker2=172.16.6.187

kubectl delete ns $tidb_cluster_ns
kubectl delete ns $operator_ns
kubectl delete -f $lvp_dir
for pv in `kubectl get pv | grep local-pv | awk '{print $1}'`
do
  kubectl delete pv/$pv
done

disk_clean () 
{
  ssh root@$1 "
  rm -f /tmp/disk_clean.sh
  echo '
  DISK_UUID=\$(blkid -s UUID -o value /dev/sda2)
  for i in \$(seq 1 3); do
    rm  -Rf /home/mnt/disks/\${DISK_UUID}_vol\${i}/*
  done
  ' &gt; /tmp/disk_clean.sh
  sh /tmp/disk_clean.sh
"
}

disk_clean $master
disk_clean $worker1
disk_clean $worker2

echo "*** finish clean ***"

kubectl create ns $tidb_cluster_ns
kubectl create ns $operator_ns

kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/crd.yaml
# 使用operator provisioner，修改路径为 /home/mnt/disks
rm -f $lvp_dir
wget https://raw.githubusercontent.com/pingcap/tidb-operator/$operator_v/manifests/local-dind/local-volume-provisioner.yaml -O $lvp_dir
sed -i "s/\/mnt\/disks/\/home\/mnt\/disks/g" $lvp_dir

# 按需修改 value
mkdir -p ${HOME}/tidb-operator &amp;&amp; \
helm inspect values pingcap/tidb-operator --version=$operator_v &gt; $operator_dir
helm install tidb-operator pingcap/tidb-operator --namespace $operator_ns --version $operator_v -f $operator_dir\
    --set operatorImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-operator:$operator_v \
    --set tidbBackupManagerImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-backup-manager:$operator_v \
    --set scheduler.kubeSchedulerImageName=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler

kubectl apply -f $lvp_dir
kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl -n $tidb_cluster_ns apply -f $tidb_cluser_file
kubectl -n $tidb_cluster_ns apply -f $tidb_monitor_file

echo "*** finsh recreate ***"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/k8s/" rel="tag"># k8s</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/23/go/cond/" rel="prev" title="sync.cond">
      <i class="fa fa-chevron-left"></i> sync.cond
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/12/tidb/pd-tso/" rel="next" title="TSO in TiDB">
      TSO in TiDB <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83"><span class="nav-number">1.</span> <span class="nav-text">0. 部署环境</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#firewallcluster1-pd-0"><span class="nav-number">1.1.</span> <span class="nav-text">firewallcluster1-pd-0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#iptables"><span class="nav-number">1.2.</span> <span class="nav-text">iptables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#disable-SELinux"><span class="nav-number">1.3.</span> <span class="nav-text">disable SELinux</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#disable-swap"><span class="nav-number">1.4.</span> <span class="nav-text">disable swap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sysctl"><span class="nav-number">1.5.</span> <span class="nav-text">sysctl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#irqbalance"><span class="nav-number">1.6.</span> <span class="nav-text">irqbalance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CPUFreq-performance"><span class="nav-number">1.7.</span> <span class="nav-text">CPUFreq performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ulimit"><span class="nav-number">1.8.</span> <span class="nav-text">ulimit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aliyun-repo"><span class="nav-number">1.9.</span> <span class="nav-text">aliyun repo</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-install-docker"><span class="nav-number">2.</span> <span class="nav-text">1. install docker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-install-kubeadm"><span class="nav-number">3.</span> <span class="nav-text">2. install kubeadm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-kubeadm-init-join"><span class="nav-number">4.</span> <span class="nav-text">3. kubeadm init&#x2F;join</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-master"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 master</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-IPv6"><span class="nav-number">4.1.1.</span> <span class="nav-text">3.1.1 IPv6</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-worker"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 worker</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E9%83%A8%E7%BD%B2-operator"><span class="nav-number">5.</span> <span class="nav-text">4. 部署 operator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E9%83%A8%E7%BD%B2-tidb-cluster1"><span class="nav-number">6.</span> <span class="nav-text">5. 部署 tidb-cluster1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%B0%83%E5%BA%A6-tikv-pod"><span class="nav-number">7.</span> <span class="nav-text">6. 调度 tikv pod</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E9%80%9A%E8%BF%87-tidb-scheduler-%E8%B0%83%E5%BA%A6"><span class="nav-number">7.1.</span> <span class="nav-text">6.1 通过 tidb-scheduler 调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E9%80%9A%E8%BF%87-topologySpreadConstraints-%E8%B0%83%E5%BA%A6"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 通过 topologySpreadConstraints 调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E9%80%9A%E8%BF%87-affinity-%E8%B0%83%E5%BA%A6"><span class="nav-number">7.3.</span> <span class="nav-text">6.3 通过 affinity 调度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-heterogeneous-%E5%9C%A8%E7%BA%BF%E8%BF%81%E7%A7%BB%E6%AD%A5%E9%AA%A4"><span class="nav-number">8.</span> <span class="nav-text">7. heterogeneous 在线迁移步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-grafana-%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E9%97%AE%E9%A2%98"><span class="nav-number">8.1.</span> <span class="nav-text">7.1 grafana 多集群监控问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="nav-number">9.</span> <span class="nav-text">9. 问题记录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#docker-change-cgroup-driver"><span class="nav-number">9.1.</span> <span class="nav-text">docker change cgroup driver</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kubeadm-init-%E6%8A%A5-nodeRegistration-name-Invalid-value"><span class="nav-number">9.2.</span> <span class="nav-text">kubeadm init 报 nodeRegistration.name: Invalid value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B7%B2%E6%9C%89-kubeadm-%E6%AE%8B%E7%95%99%E4%BF%A1%E6%81%AF"><span class="nav-number">9.3.</span> <span class="nav-text">已有 kubeadm 残留信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k8s-coredns-pod-pending"><span class="nav-number">9.4.</span> <span class="nav-text">k8s coredns pod pending</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kubectl-%E8%BF%9E%E4%B8%8D%E4%B8%8A-master-api-server"><span class="nav-number">9.5.</span> <span class="nav-text">kubectl 连不上 {master} api server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tidb-pd-pod-pending"><span class="nav-number">9.6.</span> <span class="nav-text">tidb pd pod pending</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tikv-pod-%E5%8F%8D%E5%A4%8D-CrashLoopBackOff-Error"><span class="nav-number">9.7.</span> <span class="nav-text">tikv pod 反复 CrashLoopBackOff&#x2F;Error</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tikv-pod-logs-%E6%8A%A5%E9%94%99-store-is-tombstone"><span class="nav-number">9.8.</span> <span class="nav-text">tikv pod logs 报错 store is tombstone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#operator-v1-1-12-%E9%85%8D%E7%BD%AE-topologySpreadConstraints-%E6%97%A0%E6%95%88%E6%9E%9C"><span class="nav-number">9.9.</span> <span class="nav-text">operator v1.1.12 配置 topologySpreadConstraints 无效果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#99-tidb-%E9%9B%86%E7%BE%A4%E5%88%9B%E5%BB%BA%E7%AE%80%E5%8C%96%E5%91%BD%E4%BB%A4"><span class="nav-number">10.</span> <span class="nav-text">99. tidb 集群创建简化命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mount-pv-disk"><span class="nav-number">10.1.</span> <span class="nav-text">mount pv disk</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E5%BB%BA%E9%9B%86%E7%BE%A4%E8%84%9A%E6%9C%AC-root-recreate-sh"><span class="nav-number">10.2.</span> <span class="nav-text">重建集群脚本 &#x2F;root&#x2F;recreate.sh</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">yiduoyunQ</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yiduoyunQ</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
